# SEC Forensic Auditor

## üöÄ AI/ML-Powered Forensic Auditing System for SEC 10-K Filings

A comprehensive full-stack application that performs intelligent forensic auditing of SEC 10-K filings using advanced AI/ML techniques including XGBoost, SHAP, FinBERT, and Longformer.

## üìã Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Analysis Pipeline](#analysis-pipeline)
- [Deployment](#deployment)
- [Contributing](#contributing)

## ‚ú® Features

### üîç SEC Data Extraction
- Automated extraction of 10-K filings from SEC EDGAR database
- Company search by name with CIK resolution
- Multi-filing historical analysis support

### üìä Numerical Analysis Agent
- **Financial Data Validation**: Balance sheet verification, completeness checks
- **Anomaly Detection**: Statistical outliers (Z-score), Isolation Forest, business rule violations
- **Trend Analysis**: Year-over-year growth, volatility metrics
- **Forensic Modeling**: XGBoost-based risk scoring with SHAP explainability
- **Ratio Analysis**: Profitability, leverage, liquidity, and solvency ratios

### üìù Textual Investigation Agent
- **Sentiment Analysis**: FinBERT-powered financial sentiment detection
- **Complexity Metrics**: Gunning Fog Index, obfuscation scoring
- **Red Flag Detection**: Legal terms, risk indicators, euphemisms, evasive language
- **Context Preservation**: Longformer for long-document understanding (4096 tokens)
- **Pattern Recognition**: Minimization tactics, buried bad news, excessive qualification

### üß† Chief Forensic Auditor Agent
- **Causal Inference**: Links numerical anomalies to textual disclosures
- **Risk Classification**: L1 (Low), L2 (Medium), L3 (High) with confidence scores
- **Pattern Identification**: Multi-metric decline, concentrated disclosures
- **Explainable Reports**: 4-part auditor-ready output with evidence synthesis

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ  React + TailwindCSS + Lucide Icons
‚îÇ   Dashboard     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FastAPI       ‚îÇ  Python Backend
‚îÇ   Server        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SEC EDGAR    ‚îÇ            ‚îÇ  Data           ‚îÇ
‚îÇ Extractor    ‚îÇ            ‚îÇ  Preprocessor   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                             ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                         ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Numerical   ‚îÇ      ‚îÇ  Textual         ‚îÇ
  ‚îÇ Analyst     ‚îÇ      ‚îÇ  Investigator    ‚îÇ
  ‚îÇ Agent       ‚îÇ      ‚îÇ  Agent           ‚îÇ
  ‚îÇ             ‚îÇ      ‚îÇ                  ‚îÇ
  ‚îÇ XGBoost +   ‚îÇ      ‚îÇ  FinBERT +       ‚îÇ
  ‚îÇ SHAP        ‚îÇ      ‚îÇ  Longformer      ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Chief      ‚îÇ
         ‚îÇ  Forensic   ‚îÇ
         ‚îÇ  Auditor    ‚îÇ
         ‚îÇ  Agent      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Forensic   ‚îÇ
         ‚îÇ  Report     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üõ†Ô∏è Tech Stack

### Backend
- **Framework**: FastAPI 0.109+
- **ML/AI**:
  - XGBoost 2.0+ (Numerical forensics)
  - SHAP 0.44+ (Explainability)
  - Transformers 4.37+ (FinBERT, Longformer)
  - PyTorch 2.2+
  - Scikit-learn 1.4+
- **Data Processing**: Pandas, NumPy, BeautifulSoup4
- **NLP**: NLTK, TextBlob, Sentence-Transformers
- **Database**: PostgreSQL, MongoDB (optional), Redis (optional)

### Frontend
- **Framework**: React 18+
- **Styling**: TailwindCSS 3+
- **Icons**: Lucide React
- **HTTP Client**: Fetch API

## üì¶ Installation

### Prerequisites
- Python 3.10+
- Node.js 16+ and npm
- (Optional) PostgreSQL, MongoDB, Redis for production

### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env
# Edit .env with your configuration

# Download NLTK data (first run)
python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"
```

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Create .env file
echo "REACT_APP_API_URL=http://localhost:8000/api/v1" > .env
```

## üöÄ Usage

### Start Backend Server

```bash
cd backend
python api_server.py

# Or with uvicorn
uvicorn api_server:app --host 0.0.0.0 --port 8000 --reload
```

Backend will be available at: `http://localhost:8000`

### Start Frontend

```bash
cd frontend
npm start
```

Frontend will be available at: `http://localhost:3000`

### Using the Application

1. **Search Company**: Enter a company name (e.g., "Apple", "Tesla")
2. **Select Company**: Choose from search results
3. **Analyze**: Click "Analyze" to start forensic audit
4. **Monitor Progress**: Watch real-time analysis progress
5. **Review Report**: Examine risk assessment, anomalies, recommendations
6. **Download**: Export full report as JSON

## üìö API Documentation

### Endpoints

#### `POST /api/v1/search/company`
Search for companies by name.

**Request:**
```json
{
  "company_name": "Apple Inc"
}
```

**Response:**
```json
{
  "companies": [
    {
      "cik": "0000320193",
      "name": "APPLE INC",
      "ticker": "AAPL"
    }
  ]
}
```

#### `POST /api/v1/analysis/start`
Start forensic analysis.

**Request:**
```json
{
  "cik": "0000320193",
  "filing_count": 1,
  "include_historical": true
}
```

**Response:**
```json
{
  "task_id": "task-0000320193-20240206120000",
  "status": "pending",
  "progress": 0,
  "message": "Analysis task queued"
}
```

#### `GET /api/v1/analysis/status/{task_id}`
Get analysis status.

#### `GET /api/v1/analysis/result/{task_id}`
Get completed analysis report.

#### `GET /api/v1/filings/{cik}`
List 10-K filings for a company.

#### `GET /api/v1/reports`
List all forensic reports.

## üî¨ Analysis Pipeline

### Step 1: Data Extraction
- Query SEC EDGAR database
- Download 10-K HTML documents
- Extract document sections (Item 1, 1A, 7, 7A, 8)

### Step 2: Data Preprocessing
- Clean HTML tags and formatting
- Extract financial tables
- Separate numerical and textual data
- Convert to structured JSON

### Step 3: Numerical Analysis
- **Validation**: Check data completeness and consistency
- **Anomaly Detection**:
  - Z-score outlier detection
  - Isolation Forest multivariate analysis
  - Business rule violations
- **Trend Analysis**: YoY growth, volatility, trend direction
- **Forensic Scoring**: XGBoost model with SHAP explanations
- **Output**: Quantitative report with flags, scores, SHAP values

### Step 4: Textual Analysis
- **Sentiment**: FinBERT analysis on MDA, Risk Factors
- **Complexity**: Gunning Fog Index, obfuscation metrics
- **Keyword Scanning**: Risk indicators, legal terms, euphemisms
- **Context Analysis**: Longformer for document-level understanding
- **Pattern Detection**: Evasive language, buried bad news
- **Output**: Suspicious language with sentence-level explainability

### Step 5: Synthesis & Reporting
- **Causal Inference**: Link numerical anomalies to textual disclosures
- **Risk Classification**:
  - Rule-based scoring
  - ML-based classification
  - Fusion methodology
- **Pattern Identification**: Cross-cutting patterns, anomaly clusters
- **Report Generation**: 4-part auditor-ready output
  1. Risk Level with confidence
  2. Evidence Synthesis (numerical + textual)
  3. Pattern Identification
  4. Actionable Recommendations

## üö¢ Deployment

### Production Considerations

1. **Environment Variables**: Set all secrets in production `.env`
2. **Database**: Configure PostgreSQL and MongoDB connections
3. **Redis**: Set up Redis for caching and task queue
4. **Authentication**: Implement JWT authentication (endpoints ready)
5. **Rate Limiting**: Configure SEC API rate limits
6. **Model Caching**: Pre-download ML models to cache directory
7. **Background Tasks**: Use Celery for production task queue
8. **Monitoring**: Set up logging, error tracking (Sentry)
9. **Load Balancing**: Use Nginx or similar
10. **HTTPS**: Configure SSL certificates

### Docker Deployment (Recommended)

```dockerfile
# Example Dockerfile for backend
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "api_server:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose

```yaml
version: '3.8'
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/forensic_db
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis
  
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000/api/v1
  
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=forensic_user
      - POSTGRES_PASSWORD=change_me
      - POSTGRES_DB=forensic_db
  
  redis:
    image: redis:7-alpine
```

## üîß Configuration

### Risk Thresholds
Adjust in `.env`:
```
RISK_L1_THRESHOLD=0.3   # Low risk
RISK_L2_THRESHOLD=0.6   # Medium risk
RISK_L3_THRESHOLD=0.85  # High risk
```

### XGBoost Parameters
```
XGBOOST_N_ESTIMATORS=100
XGBOOST_MAX_DEPTH=6
XGBOOST_LEARNING_RATE=0.1
```

### Model Selection
```
FINBERT_MODEL=ProsusAI/finbert
LONGFORMER_MODEL=allenai/longformer-base-4096
```

## üéØ Possible Upgrades & Enhancements

### Short-term Enhancements
1. **Real-time Monitoring**: Add WebSocket support for live progress updates
2. **Report Templates**: PDF export with professional formatting
3. **Batch Processing**: Analyze multiple companies simultaneously
4. **Email Alerts**: Notify users when high-risk filings are detected
5. **Historical Comparison**: Compare current filing against previous years
6. **Industry Benchmarking**: Compare metrics against industry averages

### Medium-term Enhancements
7. **Advanced NLP**: Add custom entity recognition for financial terms
8. **Time Series Analysis**: Implement ARIMA/LSTM for forecasting
9. **Graph Analysis**: Build knowledge graphs from disclosure networks
10. **Automated Fact-Checking**: Cross-reference claims with external data
11. **Multi-document Analysis**: Compare 10-K, 10-Q, 8-K filings
12. **Dashboard Analytics**: Advanced visualization with drill-downs

### Long-term Enhancements
13. **Real-time Data Integration**: Connect to market data APIs
14. **Peer Comparison**: Automated competitor analysis
15. **Predictive Modeling**: Forecast financial distress probability
16. **LLM Integration**: GPT-4/Claude for natural language report generation
17. **Regulatory Compliance**: Check against specific regulations (SOX, GAAP)
18. **Audit Trail**: Full forensic investigation workflow management
19. **Mobile App**: iOS/Android native applications
20. **API Marketplace**: Public API for third-party integrations

### Technical Improvements
21. **Kubernetes Deployment**: Container orchestration for scalability
22. **GraphQL API**: Alternative to REST for flexible queries
23. **Event Sourcing**: Complete audit trail with event replay
24. **Microservices**: Split agents into independent services
25. **Real-time Caching**: Intelligent cache invalidation
26. **A/B Testing**: Compare different model configurations
27. **Model Versioning**: MLOps pipeline with model registry
28. **Distributed Computing**: Spark/Dask for large-scale processing
29. **Edge Computing**: Client-side analysis for sensitive data
30. **Blockchain**: Immutable audit logs on distributed ledger

## üìä Performance Metrics

### Expected Analysis Times
- Small filing (<100 pages): 2-5 minutes
- Medium filing (100-300 pages): 5-10 minutes
- Large filing (>300 pages): 10-20 minutes

### Accuracy Metrics
- Anomaly Detection Precision: ~85-90%
- Risk Classification Accuracy: ~80-85%
- Sentiment Analysis F1-Score: ~0.87

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìÑ License

MIT License - See LICENSE file for details

## üôè Acknowledgments

- SEC EDGAR for providing public company filings
- Hugging Face for transformer models
- ProsusAI for FinBERT
- Allen Institute for AI for Longformer
- XGBoost and SHAP teams for explainable ML

## üìß Contact

For questions or support, please open an issue on GitHub.

---

**Built with ‚ù§Ô∏è using cutting-edge AI/ML technologies for financial transparency**
